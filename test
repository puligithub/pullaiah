import os
import time
import psutil
import random
import threading
import subprocess
import speedtest
import logging
import json
from flask import Flask, render_template, jsonify
from flask_socketio import SocketIO
import numpy as np
import requests
from typing import Dict, List, Any

# Configure more detailed logging
logging.basicConfig(
    level=logging.DEBUG, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='/tmp/performance_test.log'
)
logger = logging.getLogger(__name__)

class PodPerformanceStressTester:
    """
    A comprehensive performance stress testing utility for Kubernetes pods
    Provides various performance metrics and stress testing capabilities
    """
    def __init__(self, test_duration: int = 300, result_file: str = '/tmp/performance_results.json'):
        """
        Initialize the performance stress tester
        
        Args:
            test_duration (int): Duration of performance tests in seconds
            result_file (str): Path to store performance test results
        """
        self.test_duration = test_duration
        self.result_file = result_file
        self.results: Dict[str, Any] = {
            'iops_read': {},
            'iops_write': {},
            'iops_read_write': {},
            'throughput': {},
            'network': {},
            'resource_utilization': {}
        }

    def run_disk_iops_test(self, test_type: str = 'random') -> Dict[str, float]:
        """
        Perform disk IOPS (Input/Output Operations Per Second) testing
        
        Args:
            test_type (str): Type of test - 'random' or 'sequential'
        
        Returns:
            Dict with IOPS metrics
        """
        try:
            # Create a temporary test file
            test_file = f'/tmp/iops_test_{test_type}.bin'
            file_size = 1 * 1024 * 1024 * 1024  # 1GB test file
            
            # Generate random or sequential data
            if test_type == 'random':
                data = os.urandom(4096)  # 4KB random data chunks
            else:
                data = bytes(range(256)) * (4096 // 256)  # Sequential data
            
            start_time = time.time()
            write_ops = 0
            read_ops = 0
            
            # Perform write operations
            with open(test_file, 'wb') as f:
                while time.time() - start_time < self.test_duration:
                    f.write(data)
                    f.flush()
                    write_ops += 1
            
            # Perform read operations
            with open(test_file, 'rb') as f:
                while time.time() - start_time < self.test_duration * 2:
                    f.read(4096)
                    read_ops += 1
            
            # Clean up test file
            os.remove(test_file)
            
            # Calculate IOPS
            write_iops = write_ops / (time.time() - start_time)
            read_iops = read_ops / (time.time() - start_time)
            
            logger.info(f"{test_type.capitalize()} IOPS Test Results: Write IOPS={write_iops}, Read IOPS={read_iops}")
            
            return {
                'write_iops': write_iops,
                'read_iops': read_iops,
                'test_type': test_type
            }
        
        except Exception as e:
            logger.error(f"IOPS Test Failed: {e}")
            return {}

    def network_performance_test(self) -> Dict[str, float]:
        """
        Perform network performance testing
        
        Returns:
            Dict with network performance metrics
        """
        try:
            # Initialize speedtest
            st = speedtest.Speedtest()
            
            # Measure download speed
            download_speed = st.download() / 1_000_000  # Convert to Mbps
            
            # Measure upload speed
            upload_speed = st.upload() / 1_000_000  # Convert to Mbps
            
            # Measure ping
            st.get_best_server()
            ping = st.results.ping
            
            logger.info(f"Network Performance: Download={download_speed} Mbps, Upload={upload_speed} Mbps, Ping={ping} ms")
            
            return {
                'download_speed_mbps': download_speed,
                'upload_speed_mbps': upload_speed,
                'ping_ms': ping
            }
        
        except Exception as e:
            logger.error(f"Network Performance Test Failed: {e}")
            return {}

    def cpu_memory_metrics(self) -> Dict[str, float]:
        """
        Collect pod-level CPU and memory metrics
        
        Note: This method attempts to gather metrics specific to the current process
        
        Returns:
            Dict with CPU and memory utilization
        """
        try:
            # Get current process
            current_process = psutil.Process()
            
            # CPU metrics
            cpu_percent = current_process.cpu_percent(interval=1)
            
            # Memory metrics (RSS - Resident Set Size)
            memory_info = current_process.memory_info()
            memory_usage_mb = memory_info.rss / (1024 * 1024)
            
            logger.info(f"Resource Utilization: CPU={cpu_percent}%, Memory={memory_usage_mb} MB")
            
            return {
                'cpu_percent': cpu_percent,
                'memory_usage_mb': memory_usage_mb
            }
        
        except Exception as e:
            logger.error(f"Resource Metrics Collection Failed: {e}")
            return {}

    def run_comprehensive_tests(self):
        """
        Execute all performance tests and aggregate results
        """
        logger.info("Starting Comprehensive Performance Tests")
        
        # Random and Sequential IOPS Tests
        self.results['iops_read']['random'] = self.run_disk_iops_test('random')
        self.results['iops_read']['sequential'] = self.run_disk_iops_test('sequential')
        
        # Network Performance Test
        self.results['network'] = self.network_performance_test()
        
        # CPU and Memory Metrics
        self.results['resource_utilization'] = self.cpu_memory_metrics()
        
        # Save results to file
        try:
            with open(self.result_file, 'w') as f:
                json.dump(self.results, f, indent=4)
            logger.info(f"Performance test results saved to {self.result_file}")
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
        
        logger.info("Comprehensive Performance Tests Completed")

# Basic HTML Template for Performance Dashboard
performance_dashboard_html = '''
<!DOCTYPE html>
<html>
<head>
    <title>Performance Dashboard</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
</head>
<body>
    <h1>Performance Dashboard</h1>
    <div id="performance-metrics"></div>
    <script>
        fetch('/performance-data')
            .then(response => response.json())
            .then(data => {
                const metricsDiv = document.getElementById('performance-metrics');
                metricsDiv.innerHTML = `
                    <h2>Network Performance</h2>
                    <pre>${JSON.stringify(data.network, null, 2)}</pre>
                    <h2>Resource Utilization</h2>
                    <pre>${JSON.stringify(data.resource_utilization, null, 2)}</pre>
                `;
            });
    </script>
</body>
</html>
'''

# Flask Web Application for Results Visualization
app = Flask(__name__)
socketio = SocketIO(app)

@app.route('/')
def index():
    """
    Render the main performance dashboard
    """
    return performance_dashboard_html

@app.route('/performance-data')
def get_performance_data():
    """
    API endpoint to fetch performance test results
    """
    try:
        with open('/tmp/performance_results.json', 'r') as f:
            results = json.load(f)
        return jsonify(results)
    except FileNotFoundError:
        logger.warning("Performance results file not found")
        return jsonify({"error": "Performance tests not run yet"})

def run_performance_tests():
    """
    Background thread to run performance tests periodically
    """
    while True:
        try:
            logger.info("Initiating Performance Test Cycle")
            tester = PodPerformanceStressTester()
            tester.run_comprehensive_tests()
            time.sleep(300)  # Run tests every 5 minutes
        except Exception as e:
            logger.error(f"Error in performance test thread: {e}")
            time.sleep(60)  # Wait a minute before retrying

def main():
    """
    Main entry point to start performance testing and web server
    """
    try:
        # Start performance testing in a background thread
        test_thread = threading.Thread(target=run_performance_tests)
        test_thread.daemon = True
        test_thread.start()
        
        # Run Flask application
        logger.info("Starting Flask Application")
        socketio.run(app, host='0.0.0.0', port=5000)
    except Exception as e:
        logger.critical(f"Fatal error starting application: {e}")

if __name__ == '__main__':
    main()





====================

views


# OpenTelemetry Configuration for Internal Telemetry with Metric Filtering
# This configuration demonstrates how to filter internal telemetry metrics

# Global configuration
file_format: "0.3"

# Disabled by default, set to true to enable
disabled: false

# Resource configuration
resource:
  attributes:
    service.name: "internal-telemetry-service"
    service.version: "1.0.0"
    deployment.environment: "production"

# Attribute processors for internal telemetry
attribute_processor:
  # Processor to add internal telemetry labels
  internal_metrics:
    actions:
      - action: insert
        key: telemetry.source
        value: "internal"
      - action: insert
        key: component
        value: "otel-collector"

# Instrumentation configuration
instrumentation:
  general:
    # Disable automatic instrumentation that might create noise
    disabled: false
    
  # Configure specific instrumentation
  "@otel/auto-instrumentations-node":
    enabled: true
    
  # HTTP instrumentation
  "@opentelemetry/instrumentation-http":
    enabled: true
    ignore_incoming_paths:
      - /health
      - /metrics
      - /status

# Meter provider configuration with views for filtering
meter_provider:
  # Configure metric readers
  readers:
    - name: internal_metrics_reader
      pull:
        exporter:
          prometheus:
            host: "0.0.0.0"
            port: 8889
            without_units: false
            without_type_suffix: false
            without_scope_info: false
            with_resource_constant_labels:
              enabled: true
      
  # Views for filtering internal telemetry metrics
  views:
    # View 1: Allow only specific process metrics
    - selector:
        instrument_name: "process_*"
        instrument_type: gauge
      view:
        name: "process_metrics"
        description: "Process-related metrics for internal monitoring"
        aggregation:
          default: {}
        attribute_filter:
          - "process.pid"
          - "process.executable.name"
          - "process.command"
          - "process.runtime.name"
          - "process.runtime.version"
    
    # View 2: Allow memory metrics but filter out noisy attributes
    - selector:
        instrument_name: "process_memory_*"
        instrument_type: gauge
      view:
        name: "memory_metrics"
        description: "Memory usage metrics"
        aggregation:
          default: {}
        attribute_filter:
          - "type"
          - "state"
    
    # View 3: Allow CPU metrics
    - selector:
        instrument_name: "process_cpu_*"
        instrument_type: counter
      view:
        name: "cpu_metrics"
        description: "CPU usage metrics"
        aggregation:
          default: {}
        attribute_filter:
          - "state"
    
    # View 4: Allow HTTP server metrics but filter attributes
    - selector:
        instrument_name: "http_server_*"
        instrument_type: histogram
      view:
        name: "http_server_metrics"
        description: "HTTP server metrics for internal endpoints"
        aggregation:
          explicit_bucket_histogram:
            boundaries: [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 7.5, 10.0]
        attribute_filter:
          - "http.method"
          - "http.status_code"
          - "http.route"
    
    # View 5: Drop noisy client metrics
    - selector:
        instrument_name: "http_client_*"
        instrument_type: "*"
      view:
        aggregation:
          drop: {}
    
    # View 6: Allow OpenTelemetry collector internal metrics
    - selector:
        instrument_name: "otelcol_*"
        instrument_type: "*"
      view:
        name: "collector_internal_metrics"
        description: "OpenTelemetry Collector internal metrics"
        aggregation:
          default: {}
        attribute_filter:
          - "processor"
          - "receiver"
          - "exporter"
          - "service_name"
          - "service_version"
    
    # View 7: Allow only specific runtime metrics
    - selector:
        instrument_name: "runtime_*"
        instrument_type: "*"
      view:
        name: "runtime_metrics"
        description: "Runtime metrics"
        aggregation:
          default: {}
        attribute_filter:
          - "gc.action"
          - "gc.name"
          - "heap.space"
    
    # View 8: Drop all database metrics (assuming internal service doesn't need them)
    - selector:
        instrument_name: "db_*"
        instrument_type: "*"
      view:
        aggregation:
          drop: {}
    
    # View 9: Drop all external service metrics
    - selector:
        instrument_name: "external_service_*"
        instrument_type: "*"
      view:
        aggregation:
          drop: {}
    
    # View 10: Allow custom application metrics with specific prefixes
    - selector:
        instrument_name: "app_internal_*"
        instrument_type: "*"
      view:
        name: "application_internal_metrics"
        description: "Application internal metrics"
        aggregation:
          default: {}
        attribute_filter:
          - "operation"
          - "status"
          - "component"
    
    # View 11: Transform and filter JVM metrics (if applicable)
    - selector:
        instrument_name: "jvm_*"
        instrument_type: "*"
      view:
        name: "jvm_metrics"
        description: "JVM metrics for internal monitoring"
        aggregation:
          default: {}
        attribute_filter:
          - "area"
          - "pool"
          - "type"
          - "id"
    
    # View 12: Default drop view for any unmatched metrics
    - selector:
        instrument_name: "*"
        instrument_type: "*"
      view:
        aggregation:
          drop: {}

# Tracer provider configuration (minimal for internal telemetry)
tracer_provider:
  processors:
    - batch:
        timeout: 5s
        send_batch_size: 512
        send_batch_max_size: 1024
  
  # Only export internal traces
  exporters:
    - otlp:
        endpoint: "http://internal-tracing-backend:4317"
        compression: gzip
        timeout: 30s
        headers:
          api-key: "${INTERNAL_TRACING_API_KEY}"

# Logger provider configuration
logger_provider:
  processors:
    - batch:
        timeout: 5s
        send_batch_size: 512
  
  # Filter logs to only include internal telemetry
  exporters:
    - otlp:
        endpoint: "http://internal-logging-backend:4317"
        compression: gzip
        timeout: 30s
        headers:
          api-key: "${INTERNAL_LOGGING_API_KEY}"

# Sampling configuration
sampling:
  # Use parent-based sampling for internal telemetry
  parent_based:
    root:
      ratio: 0.1  # Sample 10% of internal traces
    remote_parent_sampled:
      ratio: 1.0
    remote_parent_not_sampled:
      ratio: 0.0
    local_parent_sampled:
      ratio: 1.0
    local_parent_not_sampled:
      ratio: 0.0

# Propagators configuration
propagators:
  - tracecontext
  - baggage

# Environment variables that can be used
# INTERNAL_TRACING_API_KEY: API key for internal tracing backend
# INTERNAL_LOGGING_API_KEY: API key for internal logging backend
# OTEL_EXPORTER_OTLP_ENDPOINT: Override for OTLP endpoint
# OTEL_RESOURCE_ATTRIBUTES: Additional resource attributes


===========

telemetry all metrics:


#	Metric Name	Description	Type	Metric Level
1	otelcol_exporter_enqueue_failed_ log_records	Number of logs that exporter(s) failed to enqueue.	Counter	Basic
2	otelcol_exporter_enqueue_failed_ metric_points	Number of metric points that exporter(s) failed to enqueue.	Counter	Basic
3	otelcol_exporter_enqueue_failed_ spans	Number of spans that exporter(s) failed to enqueue.	Counter	Basic
4	otelcol_exporter_queue_capacity	Fixed capacity of the sending queue, in batches.	Gauge	Basic
5	otelcol_exporter_queue_size	Current size of the sending queue, in batches.	Gauge	Basic
6	otelcol_exporter_send_failed_ log_records	Number of logs that exporter(s) failed to send to destination.	Counter	Basic
7	otelcol_exporter_send_failed_ metric_points	Number of metric points that exporter(s) failed to send to destination.	Counter	Basic
8	otelcol_exporter_send_failed_ spans	Number of spans that exporter(s) failed to send to destination.	Counter	Basic
9	otelcol_exporter_sent_log_records	Number of logs successfully sent to destination.	Counter	Basic
10	otelcol_exporter_sent_metric_points	Number of metric points successfully sent to destination.	Counter	Basic
11	otelcol_exporter_sent_spans	Number of spans successfully sent to destination.	Counter	Basic
12	otelcol_process_cpu_seconds	Total CPU user and system time in seconds.	Counter	Basic
13	otelcol_process_memory_rss	Total physical memory (resident set size) in bytes.	Gauge	Basic
14	otelcol_process_runtime_heap_ alloc_bytes	Bytes of allocated heap objects (see ‘go doc runtime.MemStats.HeapAlloc’).	Gauge	Basic
15	otelcol_process_runtime_total_ alloc_bytes	Cumulative bytes allocated for heap objects (see ‘go doc runtime.MemStats.TotalAlloc’).	Counter	Basic
16	otelcol_process_runtime_total_ sys_memory_bytes	Total bytes of memory obtained from the OS (see ‘go doc runtime.MemStats.Sys’).	Gauge	Basic
17	otelcol_process_uptime	Uptime of the process in seconds.	Counter	Basic
18	otelcol_processor_incoming_items	Number of items passed to the processor.	Counter	Basic
19	otelcol_processor_outgoing_items	Number of items emitted from the processor.	Counter	Basic
20	otelcol_receiver_accepted_ log_records	Number of logs successfully ingested and pushed into the pipeline.	Counter	Basic
21	otelcol_receiver_accepted_ metric_points	Number of metric points successfully ingested and pushed into the pipeline.	Counter	Basic
22	otelcol_receiver_accepted_spans	Number of spans successfully ingested and pushed into the pipeline.	Counter	Basic
23	otelcol_receiver_refused_ log_records	Number of logs that could not be pushed into the pipeline.	Counter	Basic
24	otelcol_receiver_refused_ metric_points	Number of metric points that could not be pushed into the pipeline.	Counter	Basic
25	otelcol_receiver_refused_spans	Number of spans that could not be pushed into the pipeline.	Counter	Basic
26	otelcol_scraper_errored_ metric_points	Number of metric points the Collector failed to scrape.	Counter	Basic
27	otelcol_scraper_scraped_ metric_points	Number of metric points scraped by the Collector.	Counter	Basic
28	otelcol_processor_batch_batch_ send_size	Number of units in the batch that was sent.	Histogram	Normal
29	otelcol_processor_batch_batch_size_ trigger_send	Number of times the batch was sent due to a size trigger.	Counter	Normal
30	otelcol_processor_batch_metadata_ cardinality	Number of distinct metadata value combinations being processed.	Counter	Normal
31	otelcol_processor_batch_timeout_ trigger_send	Number of times the batch was sent due to a timeout trigger.	Counter	Normal
32	http.client.request.body.size	Measures the size of HTTP client request bodies.	Counter	Detailed
33	http.client.request.duration	Measures the duration of HTTP client requests.	Histogram	Detailed
34	http.server.request.body.size	Measures the size of HTTP server request bodies.	Counter	Detailed
35	http.server.request.duration	Measures the duration of HTTP server requests.	Histogram	Detailed
36	http.server.response.body.size	Measures the size of HTTP server response bodies.	Counter	Detailed
37	otelcol_processor_batch_batch_ send_size_bytes	Number of bytes in the batch that was sent.	Histogram	Detailed
38	rpc.client.duration	Measures the duration of outbound RPC.	Histogram	Detailed
39	rpc.client.request.size	Measures the size of RPC request messages (uncompressed).	Histogram	Detailed
40	rpc.client.requests_per_rpc	Measures the number of messages received per RPC. Should be 1 for all non-streaming RPCs.	Histogram	Detailed
41	rpc.client.response.size	Measures the size of RPC response messages (uncompressed).	Histogram	Detailed
42	rpc.client.responses_per_rpc	Measures the number of messages sent per RPC. Should be 1 for all non-streaming RPCs.	Histogram	Detailed
43	rpc.server.duration	Measures the duration of inbound RPC.	Histogram	Detailed
44	rpc.server.request.size	Measures the size of RPC request messages (uncompressed).	Histogram	Detailed
45	rpc.server.requests_per_rpc	Measures the number of messages received per RPC. Should be 1 for all non-streaming RPCs.	Histogram	Detailed
46	rpc.server.response.size	Measures the size of RPC response messages (uncompressed).	Histogram	Detailed
47	rpc.server.responses_per_rpc	Measures the number of messages sent per RPC. Should be 1 for all non-streaming RPCs.	Histogram	Detailed
