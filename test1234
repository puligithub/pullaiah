from(bucket: "your_bucket_name")
  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
  |> filter(fn: (r) => r._measurement == "otel.receiver")
  |> filter(fn: (r) => r._field == "receiver_accepted_spans" or r._field == "receiver_accepted_metrics" or r._field == "receiver_accepted_logs")
  |> filter(fn: (r) => r.receiver != "")
  |> aggregateWindow(every: v.windowPeriod, fn: sum, createEmpty: false)
  |> yield(name: "receiver_counts")




---
apiVersion: platform.confluent.io/v1beta1
kind: KRaftController
metadata:
  name: kcontroller
  namespace: confluent
spec:
  replicas: 3  # For production, use at least 3 for high availability
  image:
    application: confluentinc/cp-server:7.9.0
    init: confluentinc/confluent-init-container:2.11.0
  dataVolumeCapacity: "100Gi"
  configOverrides:
    server:
      - process.roles=controller
      - node.id=1
      - broker.id=1
      - controller.listener.names=CONTROLLER
      - listeners=CONTROLLER://broker:29093
      - controller.quorum.voters=1@broker.confluent.svc.cluster.local:29093
      - log.dirs=/tmp/kraft-combined-logs
      - cluster.id=MkU3OEVBNTcwNTJENDM2Qk
---
apiVersion: platform.confluent.io/v1beta1
kind: Kafka
metadata:
  name: kafka
  namespace: confluent
spec:
  replicas: 3  # Adjust replica count as needed
  image:
    application: confluentinc/cp-server:7.9.0
    init: confluentinc/confluent-init-container:2.11.0
  dataVolumeCapacity: "100Gi"
  # Reference the pre-created KRaftController cluster
  configOverrides:
    server:
      # Unique node identifier for this Kafka instance
      - node.id=1
      - broker.id=1
      # Map listener names to security protocols (all PLAINTEXT in this example)
      - listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      # Endpoints advertised to clients
      - advertised.listeners=PLAINTEXT://broker.confluent.svc.cluster.local:29092,PLAINTEXT_HOST://broker.confluent.svc.cluster.local:9092
      # Topic and transaction replication settings (adjust for production)
      - offsets.topic.replication.factor=1
      - group.initial.rebalance.delay.ms=0
      - transaction.state.log.min.isr=1
      - transaction.state.log.replication.factor=1
      # JMX settings for monitoring
      - jmx.port=9101
      - jmx.hostname=localhost
      # This process runs both broker and controller roles (if desired)
      - process.roles=broker,controller
      # Controller quorum voters – should match the controller setup
      - controller.quorum.voters=1@broker:29093
      # Define the listeners for this combined process
      - listeners=PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093,PLAINTEXT_HOST://0.0.0.0:9092
      # Broker-to-broker communication uses the PLAINTEXT listener
      - inter.broker.listener.name=PLAINTEXT
      # Specify the controller listener names
      - controller.listener.names=CONTROLLER
      # Directory for log storage
      - log.dirs=/tmp/kraft-combined-logs
      # Unique cluster identifier (must match the controller’s cluster.id)
      - cluster.id=MkU3OEVBNTcwNTJENDM2Qk



$ cat claude_kraft1.yaml
apiVersion: platform.confluent.io/v1beta1
kind: KRaftController
metadata:
  name: kcontroller
  namespace: confluent
spec:
  replicas: 1  # Start with a single node for simplicity
  image:
    application: confluentinc/cp-server:7.9.0
    init: confluentinc/confluent-init-container:2.11.0
  dataVolumeCapacity: "10Gi"
  podTemplate:
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 1
        memory: 2Gi
  configOverrides:
    server:
      - process.roles=controller
      - node.id=0
      - broker.id=0
      - controller.listener.names=CONTROLLER
      - listeners=CONTROLLER://0.0.0.0:29093
      - advertised.listeners=CONTROLLER://kcontroller-0.kcontroller.confluent.svc.cluster.local:29093
      - controller.quorum.voters=0@kcontroller-0.kcontroller.confluent.svc.cluster.local:29093
      - log.dirs=/var/lib/kafka/data
      - cluster.id=MkU3OEVBNTcwNTJENDM2Qk


$ cat chatgpt_1kafka.yaml
apiVersion: platform.confluent.io/v1beta1
kind: Kafka
metadata:
  name: kafka
  namespace: confluent
spec:
  replicas: 1
  image:
    application: confluentinc/cp-server:7.9.0
    init: confluentinc/confluent-init-container:2.11.0
  dataVolumeCapacity: "10Gi"
  podTemplate:
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 1
        memory: 2Gi
  configOverrides:
    server:
      - node.id=1
      - broker.id=1
      - cluster.id=MkU3OEVBNTcwNTJENDM2Qk
      - process.roles=broker
      - listener.security.protocol.map=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
      - listeners=PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      - advertised.listeners=PLAINTEXT://kafka-0.kafka-headless.confluent.svc.cluster.local:29092,PLAINTEXT_HOST://kafka.confluent.svc.cluster.local:9092
      - bootstrap.servers=kcontroller-0.kcontroller.confluent.svc.cluster.local:29093
      - offsets.topic.replication.factor=1
      - group.initial.rebalance.delay.ms=0
      - transaction.state.log.min.isr=1
      - transaction.state.log.replication.factor=1
      - inter.broker.listener.name=PLAINTEXT
      - log.dirs=/var/lib/kafka/data
      - kraft.controller.reconnect.backoff.ms=5000
      - kraft.controller.reconnect.backoff.max.ms=30000
      - controller.socket.timeout.ms=60000
      - controller.quorum.request.timeout.ms=60000
      - confluent.telemetry.enabled=false
      - confluent.reporters.telemetry.enabled=false
      # - confluent.telemetry.reporter.bootstrap.servers=localhost:9092   <-- Removed for now
    jvm:
      - -Djava.net.preferIPv4Stack=true

============================


from(bucket: "otel-metrics")
  |> range(start: -1h)
  |> filter(fn: (r) => r._measurement == "otelcol_exporter_sent_metric_points")
  |> filter(fn: (r) => r._field == "_value")
  |> aggregateWindow(every: 1m, fn: mean)
  |> yield(name: "mean")

from(bucket: "otel-metrics")
  |> range(start: $__range)
  |> filter(fn: (r) => r._measurement == "otelcol_exporter_sent_metric_points")
  |> filter(fn: (r) => r._field == "_value")
  |> aggregateWindow(every: $__interval, fn: sum)
  |> yield(name: "sum")

 _measurement == "otelcol_exporter_send_failed_metric_points"


queue_size = from(bucket: "otel-metrics")
  |> range(start: $__range)
  |> filter(fn: (r) => r._measurement == "otelcol_exporter_queue_size")
  |> filter(fn: (r) => r._field == "_value")

queue_capacity = from(bucket: "otel-metrics")
  |> range(start: $__range)
  |> filter(fn: (r) => r._measurement == "otelcol_exporter_queue_capacity")
  |> filter(fn: (r) => r._field == "_value")

join(
  tables: {size: queue_size, capacity: queue_capacity},
  on: ["_time"]
)
|> map(fn: (r) => ({
  _time: r._time,
  _value: float(v: r._value_size) / float(v: r._value_capacity) * 100.0
}))



from(bucket: "otel-metrics")
  |> range(start: $__range)
  |> filter(fn: (r) => r._measurement == "otelcol_receiver_accepted_metric_points" or r._measurement == "otelcol_receiver_refused_metric_points")
  |> filter(fn: (r) => r._field == "_value")
  |> aggregateWindow(every: $__interval, fn: sum)
  |> group(columns: ["_measurement"])
  |> yield(name: "stacked")


from(bucket: "otel-metrics")
  |> range(start: $__range)
  |> filter(fn: (r) => r._measurement == "otelcol_processor_batch_batch_send_size")
  |> filter(fn: (r) => r._field == "_value")
  |> aggregateWindow(every: $__interval, fn: mean)



from(bucket: "otel-metrics")
  |> range(start: $__range)
  |> filter(fn: (r) =>
    r._measurement == "otelcol_fileconsumer_open_files" or
    r._measurement == "otelcol_fileconsumer_reading_files")
  |> filter(fn: (r) => r._field == "_value")
  |> aggregateWindow(every: $__interval, fn: mean)
  |> group(columns: ["_measurement"])


from(bucket: "otel-metrics")
  |> range(start: $__range)
  |> filter(fn: (r) => r._measurement == "otelcol_process_cpu_seconds")
  |> filter(fn: (r) => r._field == "_value")
  |> derivative(unit: 1s, nonNegative: true)
  |> aggregateWindow(every: $__interval, fn: mean)



sumData = from(bucket: "OtelobservabilityPoC")
  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
  |> filter(fn: (r) => r["measurement"] == "otelcol_processor_batch_batch_send_size")
  |> filter(fn: (r) => r["_field"] == "sum")
  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)

countData = from(bucket: "OtelobservabilityPoC")
  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
  |> filter(fn: (r) => r["measurement"] == "otelcol_processor_batch_batch_send_size")
  |> filter(fn: (r) => r["_field"] == "count")
  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)

join(
  tables: {sum: sumData, count: countData},
  on: ["_time"]
)
|> map(fn: (r) => ({ 
    r with 
    _value: r._value_sum / r._value_count 
}))
|> yield(name: "batch_send_size_avg")







+++++

flowchart TD
  %% Style definitions
  classDef host fill:#cce5ff,stroke:#004085,stroke-width:2,rx:10,ry:10
  classDef gateway fill:#d4edda,stroke:#155724,stroke-width:2,rx:10,ry:10
  classDef kafka fill:#fff3cd,stroke:#856404,stroke-width:2,rx:10,ry:10
  classDef influx fill:#f8d7da,stroke:#721c24,stroke-width:2,rx:10,ry:10
  classDef grafana fill:#e2e3e5,stroke:#383d41,stroke-width:2,rx:10,ry:10

  %% Nodes
  subgraph LWH["Linux & Windows Host"]
    A1[Otel Agent process]:::host
    A2[Host metrics]:::host
    A3[Otel Agent metrics<br/>(Internal telemetry)]:::host
  end

  subgraph GS["Gateway servers"]
    B1[Host Metrics]:::gateway
    B2[Internal telemetry Metrics]:::gateway
  end

  subgraph KAFKA["Kafka"]
    C1[Host metrics Kafka topics]:::kafka
    C2[Internal telemetry Kafka topics]:::kafka
  end

  subgraph INF["InfluxDB + Telegraf"]
    D1[Host Metrics Bucket]:::influx
    D2[Internal telemetry Metrics Bucket]:::influx
  end

  subgraph GRAF["Grafana GUI"]
    E1[Host Metrics]:::grafana
    E2[Agent Observability metrics]:::grafana
  end

  %% Data flow
  A1 -->|Generates| A2
  A1 -->|Generates| A3

  A2 -->|Sends to| B1
  A3 -->|Sends to| B2

  B1 -->|Publishes to| C1
  B2 -->|Publishes to| C2

  C1 -->|Streams to| D1
  C2 -->|Streams to| D2

  D1 -->|Visualized in| E1
  D2 -->|Visualized in| E2
